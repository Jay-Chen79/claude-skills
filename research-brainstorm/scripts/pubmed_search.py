#!/usr/bin/env python3
"""
PubMed æ£€ç´¢è„šæœ¬
ç”¨äºç§‘ç ”å¤´è„‘é£æš´ skill çš„å®æ—¶æŸ¥é‡åŠŸèƒ½
"""

import argparse
import json
import os
import sys
import urllib.parse
import urllib.request
from datetime import datetime
from typing import Optional


class PubMedSearcher:
    """PubMed E-utilities æ£€ç´¢å™¨"""
    
    BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def __init__(self, api_key: Optional[str] = None):
        if api_key:
            self.api_key = api_key
        else:
            # å°è¯•ä» config å¯¼å…¥ï¼Œå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            try:
                from config import PUBMED_API_KEY
                self.api_key = PUBMED_API_KEY
            except ImportError:
                self.api_key = os.environ.get("PUBMED_API_KEY", "6e303ed20473be7df617f33487f494ec4708")
    
    def search(self, query: str, max_results: int = 20) -> dict:
        """
        æ‰§è¡Œæ£€ç´¢å¹¶è¿”å›ç»“æœ
        
        Args:
            query: æ£€ç´¢å¼
            max_results: æœ€å¤§è¿”å›æ•°é‡
            
        Returns:
            åŒ…å«æ£€ç´¢ç»“æœçš„å­—å…¸
        """
        # Step 1: ESearch è·å– ID åˆ—è¡¨
        search_results = self._esearch(query, max_results)
        
        if search_results["count"] == 0:
            return {
                "query": query,
                "count": 0,
                "articles": [],
                "year_distribution": {},
                "study_types": {},
                "novelty_signal": "ğŸŸ¢",
                "novelty_reason": "æœªæ‰¾åˆ°åŒ¹é…æ–‡çŒ®"
            }
        
        # Step 2: EFetch è·å–è¯¦ç»†ä¿¡æ¯
        articles = self._efetch(search_results["ids"])
        
        # Step 3: åˆ†æç»“æœ
        analysis = self._analyze_results(articles, search_results["count"])
        
        return {
            "query": query,
            "count": search_results["count"],
            "articles": articles[:10],  # åªè¿”å›å‰10ç¯‡
            "year_distribution": analysis["year_distribution"],
            "study_types": analysis["study_types"],
            "novelty_signal": analysis["novelty_signal"],
            "novelty_reason": analysis["novelty_reason"],
            "has_recent": analysis["has_recent"],
            "has_meta": analysis["has_meta"]
        }
    
    def _esearch(self, query: str, max_results: int) -> dict:
        """æ‰§è¡Œ ESearch"""
        params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "retmode": "json",
            "sort": "relevance"
        }
        if self.api_key:
            params["api_key"] = self.api_key
        
        url = f"{self.BASE_URL}/esearch.fcgi?{urllib.parse.urlencode(params)}"
        
        try:
            with urllib.request.urlopen(url, timeout=30) as response:
                data = json.loads(response.read().decode())
                result = data.get("esearchresult", {})
                return {
                    "count": int(result.get("count", 0)),
                    "ids": result.get("idlist", [])
                }
        except Exception as e:
            print(f"ESearch é”™è¯¯: {e}", file=sys.stderr)
            return {"count": 0, "ids": []}
    
    def _efetch(self, ids: list) -> list:
        """æ‰§è¡Œ EFetch è·å–æ–‡ç« è¯¦æƒ…"""
        if not ids:
            return []
        
        params = {
            "db": "pubmed",
            "id": ",".join(ids),
            "retmode": "xml",
            "rettype": "abstract"
        }
        if self.api_key:
            params["api_key"] = self.api_key
        
        url = f"{self.BASE_URL}/efetch.fcgi?{urllib.parse.urlencode(params)}"
        
        try:
            with urllib.request.urlopen(url, timeout=30) as response:
                xml_data = response.read().decode()
                return self._parse_xml(xml_data)
        except Exception as e:
            print(f"EFetch é”™è¯¯: {e}", file=sys.stderr)
            return []
    
    def _parse_xml(self, xml_data: str) -> list:
        """ç®€å•è§£æ XML æå–æ–‡ç« ä¿¡æ¯"""
        import re
        
        articles = []
        
        # æå–æ¯ç¯‡æ–‡ç« 
        article_pattern = r'<PubmedArticle>(.*?)</PubmedArticle>'
        for match in re.finditer(article_pattern, xml_data, re.DOTALL):
            article_xml = match.group(1)
            
            # æå– PMID
            pmid_match = re.search(r'<PMID[^>]*>(\d+)</PMID>', article_xml)
            pmid = pmid_match.group(1) if pmid_match else ""
            
            # æå–æ ‡é¢˜
            title_match = re.search(r'<ArticleTitle>(.*?)</ArticleTitle>', article_xml, re.DOTALL)
            title = self._clean_text(title_match.group(1)) if title_match else ""
            
            # æå–å¹´ä»½
            year_match = re.search(r'<PubDate>.*?<Year>(\d{4})</Year>', article_xml, re.DOTALL)
            if not year_match:
                year_match = re.search(r'<DateCompleted>.*?<Year>(\d{4})</Year>', article_xml, re.DOTALL)
            year = int(year_match.group(1)) if year_match else None
            
            # æå–æ‘˜è¦
            abstract_match = re.search(r'<Abstract>(.*?)</Abstract>', article_xml, re.DOTALL)
            if abstract_match:
                abstract_text = re.sub(r'<[^>]+>', ' ', abstract_match.group(1))
                abstract = self._clean_text(abstract_text)[:500]  # æˆªå–å‰500å­—ç¬¦
            else:
                abstract = ""
            
            # æå–ä½œè€…
            authors = []
            author_pattern = r'<Author[^>]*>.*?<LastName>(.*?)</LastName>.*?<ForeName>(.*?)</ForeName>.*?</Author>'
            for author_match in re.finditer(author_pattern, article_xml, re.DOTALL):
                authors.append(f"{author_match.group(1)} {author_match.group(2)}")
            
            # æå–å‡ºç‰ˆç±»å‹
            pub_types = []
            pub_type_pattern = r'<PublicationType[^>]*>(.*?)</PublicationType>'
            for pt_match in re.finditer(pub_type_pattern, article_xml):
                pub_types.append(self._clean_text(pt_match.group(1)))
            
            articles.append({
                "pmid": pmid,
                "title": title,
                "year": year,
                "authors": authors[:3],  # åªå–å‰3ä¸ªä½œè€…
                "abstract": abstract,
                "pub_types": pub_types
            })
        
        return articles
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        import re
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        return text
    
    def _analyze_results(self, articles: list, total_count: int) -> dict:
        """åˆ†ææ£€ç´¢ç»“æœï¼Œåˆ¤æ–­æ–°é¢–æ€§"""
        current_year = datetime.now().year
        
        # å¹´ä»½åˆ†å¸ƒ
        year_distribution = {}
        for article in articles:
            if article["year"]:
                year_distribution[article["year"]] = year_distribution.get(article["year"], 0) + 1
        
        # ç ”ç©¶ç±»å‹åˆ†å¸ƒ
        study_types = {}
        for article in articles:
            for pt in article["pub_types"]:
                study_types[pt] = study_types.get(pt, 0) + 1
        
        # åˆ¤æ–­æ˜¯å¦æœ‰è¿‘æœŸæ–‡çŒ®ï¼ˆè¿‘2å¹´ï¼‰
        has_recent = any(
            article["year"] and article["year"] >= current_year - 2 
            for article in articles
        )
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ Meta åˆ†ææˆ–ç³»ç»Ÿç»¼è¿°
        meta_keywords = ["meta-analysis", "systematic review", "meta analysis"]
        has_meta = any(
            any(kw in pt.lower() for kw in meta_keywords)
            for article in articles
            for pt in article["pub_types"]
        )
        
        # åˆ¤æ–­æ–°é¢–æ€§ä¿¡å·
        if total_count > 10 and has_recent and has_meta:
            novelty_signal = "ğŸ”´"
            novelty_reason = f"é«˜åº¦é¥±å’Œï¼š{total_count}ç¯‡åŒ¹é…ï¼Œè¿‘æœŸæœ‰å‘è¡¨ï¼Œå·²æœ‰Metaåˆ†æ"
        elif total_count > 10 and has_recent:
            novelty_signal = "ğŸ”´"
            novelty_reason = f"è¾ƒä¸ºé¥±å’Œï¼š{total_count}ç¯‡åŒ¹é…ï¼Œè¿‘æœŸä»æœ‰å‘è¡¨"
        elif 1 <= total_count <= 10:
            if has_recent:
                novelty_signal = "ğŸŸ¡"
                novelty_reason = f"éƒ¨åˆ†è¦†ç›–ï¼š{total_count}ç¯‡åŒ¹é…ï¼Œæœ‰å·®å¼‚åŒ–ç©ºé—´"
            else:
                novelty_signal = "ğŸŸ¡"
                novelty_reason = f"éƒ¨åˆ†è¦†ç›–ï¼š{total_count}ç¯‡åŒ¹é…ï¼Œä½†æ–‡çŒ®è¾ƒè€ï¼Œå¯èƒ½æœ‰æ›´æ–°æœºä¼š"
        else:
            novelty_signal = "ğŸŸ¢"
            novelty_reason = "ç›¸å¯¹ç©ºç™½ï¼šåŒ¹é…æ–‡çŒ®å¾ˆå°‘ï¼Œéœ€éªŒè¯ç”Ÿç‰©å­¦åˆç†æ€§"
        
        return {
            "year_distribution": year_distribution,
            "study_types": study_types,
            "has_recent": has_recent,
            "has_meta": has_meta,
            "novelty_signal": novelty_signal,
            "novelty_reason": novelty_reason
        }


def generate_search_query(concepts: list, operator: str = "AND") -> str:
    """
    æ ¹æ®æ¦‚å¿µåˆ—è¡¨ç”Ÿæˆæ£€ç´¢å¼
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨ï¼Œæ¯ä¸ªæ¦‚å¿µå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åŒä¹‰è¯åˆ—è¡¨
        operator: æ¦‚å¿µé—´çš„é€»è¾‘è¿ç®—ç¬¦
        
    Returns:
        æ£€ç´¢å¼å­—ç¬¦ä¸²
    """
    terms = []
    for concept in concepts:
        if isinstance(concept, list):
            # åŒä¹‰è¯ç”¨ OR è¿æ¥
            term = "(" + " OR ".join(concept) + ")"
        else:
            term = concept
        terms.append(term)
    
    return f" {operator} ".join(terms)


def main():
    parser = argparse.ArgumentParser(description="PubMed æ£€ç´¢å·¥å…·")
    parser.add_argument("--query", "-q", required=True, help="æ£€ç´¢å¼")
    parser.add_argument("--api_key", "-k", help="PubMed API Key")
    parser.add_argument("--max_results", "-n", type=int, default=20, help="æœ€å¤§è¿”å›æ•°é‡")
    parser.add_argument("--output", "-o", choices=["json", "text"], default="text", help="è¾“å‡ºæ ¼å¼")
    
    args = parser.parse_args()
    
    searcher = PubMedSearcher(api_key=args.api_key)
    results = searcher.search(args.query, args.max_results)
    
    if args.output == "json":
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        print(f"\n{'='*60}")
        print(f"æ£€ç´¢å¼: {results['query']}")
        print(f"åŒ¹é…æ•°é‡: {results['count']}")
        print(f"æ–°é¢–æ€§ä¿¡å·: {results['novelty_signal']}")
        print(f"åˆ¤æ–­ç†ç”±: {results['novelty_reason']}")
        print(f"{'='*60}\n")
        
        if results["articles"]:
            print("ç›¸å…³æ–‡çŒ®:\n")
            for i, article in enumerate(results["articles"], 1):
                authors_str = ", ".join(article["authors"]) if article["authors"] else "Unknown"
                print(f"{i}. [{article['year'] or 'N/A'}] {article['title']}")
                print(f"   ä½œè€…: {authors_str}")
                print(f"   PMID: {article['pmid']}")
                if article["abstract"]:
                    print(f"   æ‘˜è¦: {article['abstract'][:200]}...")
                print()
        
        if results["year_distribution"]:
            print("å¹´ä»½åˆ†å¸ƒ:", dict(sorted(results["year_distribution"].items())))
        
        if results["study_types"]:
            print("ç ”ç©¶ç±»å‹:", results["study_types"])


if __name__ == "__main__":
    main()
