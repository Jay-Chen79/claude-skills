#!/usr/bin/env python3
"""
Medical Research AI - ä¸»æµç¨‹ç¼–æ’å™¨
æ•´åˆæ–‡çŒ®æ£€ç´¢ã€Gapåˆ†æã€ç ”ç©¶è®¾è®¡ç”Ÿæˆï¼Œå®ç°ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–
"""

import sys
import json
from typing import Dict, List, Optional

# å¯¼å…¥å„æ¨¡å—
from pubmed_search import PubMedSearcher
from gap_analyzer import GapAnalyzer
from study_designer import StudyDesigner


class MedicalResearchAI:
    """åŒ»å­¦ç§‘ç ”å…¨æµç¨‹è‡ªåŠ¨åŒ–ç³»ç»Ÿ"""

    def __init__(self, api_key: str = "", email: str = ""):
        """
        Args:
            api_key: NCBI API key (å¯é€‰ï¼Œæé«˜è¯·æ±‚é…é¢)
            email: è”ç³»é‚®ç®± (NCBI è¦æ±‚)
        """
        self.searcher = PubMedSearcher(api_key=api_key, email=email)
        self.current_analysis = None

    def run_full_workflow(self, topic: str, keywords: List[str],
                          mesh_terms: List[str] = None,
                          population: str = "æˆäºº",
                          study_type_preference: str = None,
                          max_papers: int = 500) -> Dict:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµ

        Args:
            topic: ç ”ç©¶ä¸»é¢˜ï¼ˆå¦‚ï¼šå¹²çœ¼ç—‡ã€è¿‘è§†ã€é’å…‰çœ¼ï¼‰
            keywords: å…³é”®è¯åˆ—è¡¨
            mesh_terms: MeSH è¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            population: ç›®æ ‡äººç¾¤ï¼ˆæˆäºº/å„¿ç«¥é’å°‘å¹´/è€å¹´äººï¼‰
            study_type_preference: ç ”ç©¶ç±»å‹åå¥½
            max_papers: æœ€å¤§æ£€ç´¢æ–‡çŒ®æ•°

        Returns:
            å®Œæ•´åˆ†æç»“æœå­—å…¸
        """
        print("\n" + "="*70)
        print("ğŸ”¬ Medical Research AI - åŒ»å­¦ç§‘ç ”å…¨æµç¨‹è‡ªåŠ¨åŒ–")
        print("="*70)

        # ============ Phase 1: æ–‡çŒ®æ£€ç´¢ ============
        print("\nğŸ“š Phase 1: å¤§è§„æ¨¡æ–‡çŒ®æ£€ç´¢...")
        print(f"   ç ”ç©¶ä¸»é¢˜: {topic}")
        print(f"   ç›®æ ‡äººç¾¤: {population}")

        query = self.searcher.build_ophthalmology_query(
            keywords=keywords,
            mesh_terms=mesh_terms,
            study_type=study_type_preference
        )

        print(f"   æ£€ç´¢å¼: {query[:100]}...")

        pmids = self.searcher.search(query, max_results=max_papers)
        print(f"   âœ“ æ£€ç´¢åˆ° {len(pmids)} ç¯‡ç›¸å…³æ–‡çŒ®")

        if not pmids:
            print("   âš ï¸ æœªæ£€ç´¢åˆ°æ–‡çŒ®ï¼Œè¯·è°ƒæ•´æ£€ç´¢è¯")
            return {"error": "æœªæ£€ç´¢åˆ°æ–‡çŒ®"}

        # ============ Phase 2: è·å–æ–‡çŒ®æ‘˜è¦ ============
        print(f"\nğŸ“– Phase 2: è·å–æ–‡çŒ®æ‘˜è¦...")
        summaries = self.searcher.fetch_summaries(pmids)
        print(f"   âœ“ æˆåŠŸè·å– {len(summaries)} ç¯‡æ‘˜è¦")

        # ============ Phase 3: æ–‡çŒ®åˆ†æ ============
        print(f"\nğŸ“Š Phase 3: æ–‡çŒ®æ·±åº¦åˆ†æ...")

        pub_trend = self.searcher.analyze_publication_dates(summaries)
        study_types = self.searcher.analyze_study_types(summaries)

        print(f"   å‘è¡¨æ—¶é—´èŒƒå›´: {pub_trend.get('year_range', 'N/A')}")
        print(f"   æ€»æ–‡çŒ®æ•°: {pub_trend.get('total', 0)}")
        print(f"   è¿‘5å¹´å‘è¡¨: {pub_trend.get('recent_5_years', 0)} ç¯‡")

        print(f"\n   ç ”ç©¶ç±»å‹åˆ†å¸ƒ:")
        for st, count in study_types.items():
            if count > 0:
                print(f"     â€¢ {st}: {count} ç¯‡")

        # ============ Phase 4: Gap åˆ†æ ============
        print(f"\nğŸ” Phase 4: è¯†åˆ«ç ”ç©¶ç©ºç™½...")
        analyzer = GapAnalyzer(summaries)
        gaps = analyzer.identify_gaps()

        print(f"   âœ“ è¯†åˆ«å‡º {len(gaps)} ä¸ªç ”ç©¶ç©ºç™½")

        # ============ Phase 5: ç”Ÿæˆç ”ç©¶æ–¹å‘ ============
        print(f"\nğŸ’¡ Phase 5: ç”Ÿæˆç ”ç©¶æ–¹å‘...")
        directions = analyzer.generate_research_directions(topic)

        print(f"   âœ“ ç”Ÿæˆ {len(directions)} ä¸ªç ”ç©¶æ–¹å‘")

        # ============ Phase 6: ç”Ÿæˆè¯¦ç»†æ–¹æ¡ˆ ============
        print(f"\nğŸ“‹ Phase 6: ç”Ÿæˆè¯¦ç»†ç ”ç©¶æ–¹æ¡ˆ...")

        protocols = []
        for i, direction in enumerate(directions[:3], 1):  # å– top 3
            print(f"   [{i}] {direction['title']}")
            designer = StudyDesigner(
                topic=topic,
                study_design=direction['design'],
                population=direction.get('population', population)
            )
            protocol = designer.generate_full_protocol()
            protocol['innovation'] = direction.get('innovation', '')
            protocol['innovation_score'] = direction.get('innovation_score', '')
            protocols.append(protocol)

        # ============ Phase 7: ç”Ÿæˆä¼¦ç†ææ–™ ============
        print(f"\nâš–ï¸  Phase 7: ç”Ÿæˆä¼¦ç†ç”³è¯·ææ–™...")

        ethics_materials = []
        for protocol in protocols:
            ethics = self._generate_ethics_materials(protocol)
            ethics_materials.append(ethics)

        # ============ æ±‡æ€»ç»“æœ ============
        self.current_analysis = {
            "topic": topic,
            "population": population,
            "literature_search": {
                "query": query,
                "total_found": len(pmids),
                "abstracts_obtained": len(summaries),
                "publication_trend": pub_trend,
                "study_types": study_types,
            },
            "gap_analysis": {
                "gaps_identified": gaps,
                "total_gaps": len(gaps),
            },
            "research_directions": directions,
            "detailed_protocols": protocols,
            "ethics_materials": ethics_materials,
        }

        print(f"\nâœ… å·¥ä½œæµå®Œæˆï¼")

        return self.current_analysis

    def _generate_ethics_materials(self, protocol: Dict) -> Dict:
        """ç”Ÿæˆä¼¦ç†ç”³è¯·ææ–™"""
        basic_info = protocol['basic_info']

        return {
            "ä¼¦ç†ç”³è¯·è¡¨": {
                "é¡¹ç›®åç§°": basic_info['title'],
                "ç ”ç©¶ç±»å‹": basic_info['study_design'],
                "ç ”ç©¶è€…": "[å¾…å¡«å†™]",
                "å•ä½": "[å¾…å¡«å†™]",
                "ç ”ç©¶æœŸé™": f"{sum(p['duration'] for p in protocol['timeline'])}ä¸ªæœˆ",
                "ç ”ç©¶ç›®çš„": protocol['objectives']['primary'],
                "ç ”ç©¶äººç¾¤": basic_info['population'],
                "æ ·æœ¬é‡": protocol['sample_size'].get('with_attrition', protocol['sample_size'].get('calculated_n', 'å¾…å®š')),
                "ä¸»è¦é£é™©": protocol['ethics']['risk_benefit']['risks'],
                "é¢„æœŸè·ç›Š": protocol['ethics']['risk_benefit']['benefits'],
            },
            "çŸ¥æƒ…åŒæ„ä¹¦": {
                "æ ‡é¢˜": f"{basic_info['title']}çŸ¥æƒ…åŒæ„ä¹¦",
                "ç ”ç©¶è¯´æ˜": self._generate_consent_content(protocol),
                "é£é™©å‘ŠçŸ¥": protocol['ethics']['risk_benefit']['risks'],
                "è·ç›Šè¯´æ˜": protocol['ethics']['risk_benefit']['benefits'],
                "è‡ªæ„¿å£°æ˜": "æˆ‘å·²å……åˆ†äº†è§£æœ¬ç ”ç©¶çš„å†…å®¹å’Œé£é™©ï¼Œè‡ªæ„¿å‚åŠ ç ”ç©¶ã€‚",
                "è”ç³»æ–¹å¼": "[å¾…å¡«å†™ç ”ç©¶è€…è”ç³»æ–¹å¼]",
            },
        }

    def _generate_consent_content(self, protocol: Dict) -> str:
        """ç”ŸæˆçŸ¥æƒ…åŒæ„ä¹¦æ­£æ–‡"""
        return f"""
æœ¬ç ”ç©¶æ—¨åœ¨{protocol['objectives']['primary']}ã€‚

ç ”ç©¶å¤§çº¦éœ€è¦{sum(p['duration'] for p in protocol['timeline'])}ä¸ªæœˆå®Œæˆï¼Œ
éœ€è¦æ‚¨é…åˆå®Œæˆä»¥ä¸‹æ£€æŸ¥ï¼š
{', '.join(protocol['study_procedures']['baseline'][:5])}ç­‰ã€‚

å‚ä¸æœ¬ç ”ç©¶çš„é£é™©åŒ…æ‹¬ï¼š{'; '.join(protocol['ethics']['risk_benefit']['risks'])}ã€‚

å‚ä¸æœ¬ç ”ç©¶çš„è·ç›ŠåŒ…æ‹¬ï¼š{'; '.join(protocol['ethics']['risk_benefit']['benefits'])}ã€‚

æ‚¨çš„æ‰€æœ‰èµ„æ–™éƒ½å°†ä¸¥æ ¼ä¿å¯†ï¼Œç ”ç©¶ç»“æœå°†ç”¨äºç§‘å­¦å‘è¡¨ã€‚
æ‚¨æœ‰æƒåœ¨ä»»ä½•æ—¶å€™é€€å‡ºç ”ç©¶ï¼Œè¿™ä¸ä¼šå½±å“æ‚¨çš„æ­£å¸¸æ²»ç–—ã€‚
"""

    def print_summary_report(self):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        if not self.current_analysis:
            print("è¯·å…ˆè¿è¡Œ run_full_workflow()")
            return

        analysis = self.current_analysis

        print("\n\n" + "="*70)
        print("ğŸ“Š Medical Research AI - åˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("="*70)

        print(f"\nç ”ç©¶ä¸»é¢˜: {analysis['topic']}")
        print(f"ç›®æ ‡äººç¾¤: {analysis['population']}")

        print(f"\n{'â”€'*70}")
        print("ğŸ“š æ–‡çŒ®æ£€ç´¢ç»“æœ")
        print(f"{'â”€'*70}")
        lit = analysis['literature_search']
        print(f"æ£€ç´¢æ–‡çŒ®æ•°: {lit['total_found']} ç¯‡")
        print(f"è·å–æ‘˜è¦: {lit['abstracts_obtained']} ç¯‡")
        print(f"å‘è¡¨å¹´ä»½: {lit['publication_trend'].get('year_range', 'N/A')}")
        print(f"è¿‘5å¹´å‘è¡¨: {lit['publication_trend'].get('recent_5_years', 0)} ç¯‡")

        print(f"\nç ”ç©¶ç±»å‹åˆ†å¸ƒ:")
        for st, count in lit['study_types'].items():
            if count > 0:
                print(f"  â€¢ {st}: {count} ç¯‡ ({count/lit['abstracts_obtained']*100:.1f}%)")

        print(f"\n{'â”€'*70}")
        print("ğŸ” è¯†åˆ«çš„ç ”ç©¶ç©ºç™½")
        print(f"{'â”€'*70}")
        for i, gap in enumerate(analysis['gap_analysis']['gaps_identified'][:5], 1):
            print(f"\n{i}. [{gap['type']}] {gap['gap']}")
            print(f"   è¯æ®: {gap['evidence']}")
            print(f"   æ½œåŠ›: {gap['potential']}")

        print(f"\n{'â”€'*70}")
        print("ğŸ’¡ æ¨èç ”ç©¶æ–¹å‘ (æŒ‰åˆ›æ–°æ€§æ’åº)")
        print(f"{'â”€'*70}")
        for i, direction in enumerate(analysis['research_directions'][:5], 1):
            print(f"\n{i}. {direction['title']}")
            print(f"   è®¾è®¡: {direction['design']}")
            print(f"   åˆ›æ–°æ€§: {direction['innovation_score']}")
            print(f"   å¯è¡Œæ€§: {direction['feasibility']}")
            print(f"   æè¿°: {direction['description']}")

        print(f"\n{'â”€'*70}")
        print("ğŸ“‹ è¯¦ç»†ç ”ç©¶æ–¹æ¡ˆ (Top 3)")
        print(f"{'â”€'*70}")
        for i, protocol in enumerate(analysis['detailed_protocols'], 1):
            print(f"\nã€æ–¹æ¡ˆ {i}ã€‘{protocol['basic_info']['title']}")
            print(f"  è®¾è®¡ç±»å‹: {protocol['basic_info']['study_design']}")
            print(f"  ç ”ç©¶äººç¾¤: {protocol['basic_info']['population']}")
            print(f"  ä¸»è¦ç»ˆç‚¹: {protocol['endpoints']['primary_endpoint']}")
            print(f"  æ ·æœ¬é‡: {protocol['sample_size']['with_attrition']}")
            print(f"  åˆ›æ–°æ€§: {protocol['innovation_score']}")
            print(f"  åˆ›æ–°ç‚¹: {protocol['innovation']}")

        print(f"\n{'â”€'*70}")
        print("âš–ï¸  ä¼¦ç†ææ–™")
        print(f"{'â”€'*70}")
        print("å·²ç”Ÿæˆä»¥ä¸‹ææ–™:")
        for i, ethics in enumerate(analysis['ethics_materials'], 1):
            print(f"  {i}. æ–¹æ¡ˆ{i}: ä¼¦ç†ç”³è¯·è¡¨ã€çŸ¥æƒ…åŒæ„ä¹¦")

        print("\n" + "="*70)
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print("="*70)

    def save_to_json(self, filepath: str = None):
        """ä¿å­˜åˆ†æç»“æœåˆ° JSON æ–‡ä»¶"""
        if not self.current_analysis:
            print("è¯·å…ˆè¿è¡Œ run_full_workflow()")
            return

        if filepath is None:
            topic = self.current_analysis['topic']
            filepath = f"research_analysis_{topic.replace(' ', '_')}.json"

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.current_analysis, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜è‡³: {filepath}")

    def generate_markdown_report(self, filepath: str = None) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        if not self.current_analysis:
            print("è¯·å…ˆè¿è¡Œ run_full_workflow()")
            return ""

        analysis = self.current_analysis
        topic = analysis['topic']

        md_content = f"""# {topic} - ç§‘ç ”é¡¹ç›®è‡ªåŠ¨åŒ–åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {self._get_current_time()}
**ç›®æ ‡äººç¾¤**: {analysis['population']}

---

## ğŸ“š ä¸€ã€æ–‡çŒ®æ£€ç´¢ç»“æœ

### 1.1 æ£€ç´¢æ¦‚å†µ

| é¡¹ç›® | æ•°å€¼ |
|------|------|
| æ£€ç´¢æ–‡çŒ®æ•° | {analysis['literature_search']['total_found']} ç¯‡ |
| è·å–æ‘˜è¦æ•° | {analysis['literature_search']['abstracts_obtained']} ç¯‡ |
| å‘è¡¨å¹´ä»½èŒƒå›´ | {analysis['literature_search']['publication_trend'].get('year_range', 'N/A')} |
| è¿‘5å¹´å‘è¡¨ | {analysis['literature_search']['publication_trend'].get('recent_5_years', 0)} ç¯‡ |

### 1.2 ç ”ç©¶ç±»å‹åˆ†å¸ƒ

"""

        # ç ”ç©¶ç±»å‹è¡¨æ ¼
        total = analysis['literature_search']['abstracts_obtained']
        for st, count in analysis['literature_search']['study_types'].items():
            if count > 0:
                md_content += f"- **{st}**: {count} ç¯‡ ({count/total*100:.1f}%)\n"

        md_content += f"""

---

## ğŸ” äºŒã€ç ”ç©¶ç©ºç™½åˆ†æ

å…±è¯†åˆ« **{analysis['gap_analysis']['total_gaps']}** ä¸ªç ”ç©¶ç©ºç™½ï¼š

"""

        for i, gap in enumerate(analysis['gap_analysis']['gaps_identified'], 1):
            md_content += f"""
### {i}. {gap['gap']}

- **ç±»å‹**: {gap['type']}
- **è¯æ®**: {gap['evidence']}
- **ç ”ç©¶æ½œåŠ›**: {gap['potential']}
"""

        md_content += """

---

## ğŸ’¡ ä¸‰ã€æ¨èç ”ç©¶æ–¹å‘

"""

        for i, direction in enumerate(analysis['research_directions'], 1):
            md_content += f"""
### {i}. {direction['title']}

- **ç ”ç©¶è®¾è®¡**: {direction['design']}
- **ç›®æ ‡äººç¾¤**: {direction.get('population', 'æˆäºº')}
- **åˆ›æ–°æ€§**: {direction['innovation_score']}
- **å¯è¡Œæ€§**: {direction['feasibility']}
- **ç ”ç©¶æè¿°**: {direction['description']}
- **åˆ›æ–°ç‚¹**: {direction['innovation']}
"""

        md_content += """

---

## ğŸ“‹ å››ã€è¯¦ç»†ç ”ç©¶æ–¹æ¡ˆ

"""

        for i, protocol in enumerate(analysis['detailed_protocols'], 1):
            md_content += f"""
### æ–¹æ¡ˆ {i}: {protocol['basic_info']['title']}

**åŸºæœ¬ä¿¡æ¯**
- ç ”ç©¶è®¾è®¡: {protocol['basic_info']['study_design']}
- ç ”ç©¶äººç¾¤: {protocol['basic_info']['population']}
- ç ”ç©¶ä¸­å¿ƒ: {protocol['basic_info']['research_center']}

**ç ”ç©¶ç›®çš„**
- ä¸»è¦ç›®çš„: {protocol['objectives']['primary']}

**ç ”ç©¶ç»ˆç‚¹**
- ä¸»è¦ç»ˆç‚¹: {protocol['endpoints']['primary_endpoint']}
- æ¬¡è¦ç»ˆç‚¹: {', '.join(protocol['endpoints']['secondary_endpoints'][:3])}

**æ ·æœ¬é‡**
- è®¡ç®—æ ·æœ¬é‡: {protocol['sample_size'].get('calculated_n', 'N/A')}
- è€ƒè™‘å¤±è®¿: {protocol['sample_size'].get('with_attrition', 'N/A')}

**çº³å…¥æ ‡å‡†**
"""
            for criteria in protocol['inclusion_exclusion']['inclusion'][:5]:
                md_content += f"- {criteria}\n"

            md_content += "**æ’é™¤æ ‡å‡†**\n"
            for criteria in protocol['inclusion_exclusion']['exclusion'][:5]:
                md_content += f"- {criteria}\n"

        md_content += """

---

## âš–ï¸ äº”ã€ä¼¦ç†ç”³è¯·ææ–™

å·²ç”Ÿæˆä»¥ä¸‹ææ–™ï¼Œå¯ç”¨äºä¼¦ç†å§”å‘˜ä¼šç”³è¯·ï¼š
- ä¼¦ç†ç”³è¯·è¡¨
- çŸ¥æƒ…åŒæ„ä¹¦æ¨¡æ¿
- ç ”ç©¶æ–¹æ¡ˆæ‘˜è¦

---

*æœ¬æŠ¥å‘Šç”± Medical Research AI è‡ªåŠ¨ç”Ÿæˆï¼Œå»ºè®®ç”±ä¸“ä¸šç ”ç©¶äººå‘˜å®¡æ ¸åä½¿ç”¨ã€‚*
"""

        if filepath:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ä¿å­˜è‡³: {filepath}")

        return md_content

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="Medical Research AI - åŒ»å­¦ç§‘ç ”å…¨æµç¨‹è‡ªåŠ¨åŒ–")
    parser.add_argument("--topic", required=True, help="ç ”ç©¶ä¸»é¢˜ï¼ˆå¦‚ï¼šå¹²çœ¼ç—‡ã€è¿‘è§†ï¼‰")
    parser.add_argument("--keywords", nargs="+", required=True, help="æ£€ç´¢å…³é”®è¯")
    parser.add_argument("--mesh", nargs="*", help="MeSH è¯")
    parser.add_argument("--population", default="æˆäºº", help="ç›®æ ‡äººç¾¤")
    parser.add_argument("--max-papers", type=int, default=500, help="æœ€å¤§æ£€ç´¢æ–‡çŒ®æ•°")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # è¿è¡Œåˆ†æ
    ai = MedicalResearchAI()
    result = ai.run_full_workflow(
        topic=args.topic,
        keywords=args.keywords,
        mesh_terms=args.mesh,
        population=args.population,
        max_papers=args.max_papers
    )

    # æ‰“å°æŠ¥å‘Š
    ai.print_summary_report()

    # ä¿å­˜ç»“æœ
    if args.output:
        ai.save_to_json(args.output)
        md_path = args.output.replace('.json', '.md')
        ai.generate_markdown_report(md_path)


if __name__ == "__main__":
    # ç¤ºä¾‹è¿è¡Œ
    if len(sys.argv) == 1:
        print("Medical Research AI - åŒ»å­¦ç§‘ç ”å…¨æµç¨‹è‡ªåŠ¨åŒ–")
        print("\nç¤ºä¾‹: åˆ†æå¹²çœ¼ç—‡ä¸å±å¹•æ—¶é—´çš„å…³ç³»")
        print("-" * 50)

        ai = MedicalResearchAI()
        result = ai.run_full_workflow(
            topic="å¹²çœ¼ç—‡ä¸å±å¹•æ—¶é—´",
            keywords=["dry eye", "screen time", "digital device", "computer vision syndrome"],
            mesh_terms=["Dry Eye Syndromes"],
            population="æˆäºº",
            max_papers=100  # ç¤ºä¾‹ç”¨å°‘é‡æ–‡çŒ®
        )

        ai.print_summary_report()
        ai.save_to_json()
        ai.generate_markdown_report()

    else:
        main()
